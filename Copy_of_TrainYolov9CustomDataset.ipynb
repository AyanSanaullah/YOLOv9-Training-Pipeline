{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNRA22QGKEZ8"
      },
      "outputs": [],
      "source": [
        "### 0. Mount Google Drive ###\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(#your file path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 1. Prepare data ###\n",
        "\n",
        "!scp #your data zip file path\n",
        "\n",
        "!unzip #your data unzipped file path"
      ],
      "metadata": {
        "id": "kd1E7c4pKFYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 2. Clone repository ###\n",
        "\n",
        "!git clone https://github.com/computervisioneng/yolov9\n",
        "\n",
        "%cd yolov9"
      ],
      "metadata": {
        "id": "73Vb0XJGKIkM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "034905d6-6326-4559-ec58-02aabe19b3d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov9'...\n",
            "remote: Enumerating objects: 397, done.\u001b[K\n",
            "remote: Total 397 (delta 0), reused 0 (delta 0), pack-reused 397 (from 1)\u001b[K\n",
            "Receiving objects: 100% (397/397), 2.55 MiB | 9.42 MiB/s, done.\n",
            "Resolving deltas: 100% (200/200), done.\n",
            "/content/yolov9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### 3. Install requirements ###\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "8otSfK3gKJvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 4. Train model ###\n",
        "\n",
        "from yolov9 import Yolov9\n",
        "\n",
        "model = Yolov9('object-detection')\n",
        "\n",
        "model.train(data= # your file path to data.yaml file , epochs=20)"
      ],
      "metadata": {
        "id": "u9Hh-_jRKLn4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "043d6fa5-baaa-4e0b-d5f0-1fc6b1e9bf51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose \"Don't visualize my results\"\n",
            "\u001b[34m\u001b[1mtrain_dual: \u001b[0mweights=, cfg=models/detect/yolov9-c.yaml, data=/content/gdrive/My Drive/Yolo_V9/data.yaml, hyp=hyp.scratch-high.yaml, epochs=20, batch_size=8, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=yolov9-c, exist_ok=False, quad=False, cos_lr=False, flat_cos_lr=False, fixed_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, min_items=0, close_mosaic=15, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "YOLO 🚀 34ba27f Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, cls_pw=1.0, obj=0.7, obj_pw=1.0, dfl=1.5, iou_t=0.2, anchor_t=5.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.9, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.15, copy_paste=0.3\n",
            "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLO 🚀 in ClearML\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLO 🚀 runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "100%|██████████| 755k/755k [00:00<00:00, 3.16MB/s]\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1         0  models.common.Silence                   []                            \n",
            "  1                -1  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
            "  2                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  3                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
            "  4                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
            "  5                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
            "  6                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            "  7                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            "  8                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            "  9                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            " 10                -1  1    656896  models.common.SPPELAN                   [512, 512, 256]               \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 7]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
            " 14                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 15           [-1, 5]  1         0  models.common.Concat                    [1]                           \n",
            " 16                -1  1    912640  models.common.RepNCSPELAN4              [1024, 256, 256, 128, 1]      \n",
            " 17                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
            " 18          [-1, 13]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1   2988544  models.common.RepNCSPELAN4              [768, 512, 512, 256, 1]       \n",
            " 20                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            " 21          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 22                -1  1   3119616  models.common.RepNCSPELAN4              [1024, 512, 512, 256, 1]      \n",
            " 23                 5  1    131328  models.common.CBLinear                  [512, [256]]                  \n",
            " 24                 7  1    393984  models.common.CBLinear                  [512, [256, 512]]             \n",
            " 25                 9  1    656640  models.common.CBLinear                  [512, [256, 512, 512]]        \n",
            " 26                 0  1      1856  models.common.Conv                      [3, 64, 3, 2]                 \n",
            " 27                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            " 28                -1  1    212864  models.common.RepNCSPELAN4              [128, 256, 128, 64, 1]        \n",
            " 29                -1  1    164352  models.common.ADown                     [256, 256]                    \n",
            " 30  [23, 24, 25, -1]  1         0  models.common.CBFuse                    [[0, 0, 0]]                   \n",
            " 31                -1  1    847616  models.common.RepNCSPELAN4              [256, 512, 256, 128, 1]       \n",
            " 32                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            " 33      [24, 25, -1]  1         0  models.common.CBFuse                    [[1, 1]]                      \n",
            " 34                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            " 35                -1  1    656384  models.common.ADown                     [512, 512]                    \n",
            " 36          [25, -1]  1         0  models.common.CBFuse                    [[2]]                         \n",
            " 37                -1  1   2857472  models.common.RepNCSPELAN4              [512, 512, 512, 256, 1]       \n",
            " 38[31, 34, 37, 16, 19, 22]  1  21542822  models.yolo.DualDDetect                 [1, [512, 512, 512, 256, 512, 512]]\n",
            "yolov9-c summary: 962 layers, 50999590 parameters, 50999558 gradients, 238.9 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 238 weight(decay=0.0), 255 weight(decay=0.0005), 253 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, num_output_channels=3, method='weighted_average'), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/data/labels/train... 737 images, 1 backgrounds, 2 corrupt: 100%|██████████| 737/737 00:00\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/images/train/000339.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0234]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ /content/data/images/train/000523.jpg: ignoring corrupt image/label: negative label values [ -0.0058594]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/data/labels/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/data/labels/test... 737 images, 1 backgrounds, 2 corrupt: 100%|██████████| 737/737 00:00\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/data/images/test/000339.jpg: ignoring corrupt image/label: non-normalized or out of bounds coordinates [     1.0234]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ /content/data/images/test/000523.jpg: ignoring corrupt image/label: negative label values [ -0.0058594]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/data/labels/test.cache\n",
            "Plotting labels to runs/train/yolov9-c/labels.jpg... \n",
            "/content/yolov9/train_dual.py:255: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/yolov9-c\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "  0%|          | 0/92 00:00/content/yolov9/train_dual.py:313: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "       0/19      9.99G      6.127      4.972      5.208        307        640:   0%|          | 0/92 00:05Exception in thread Thread-22 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "WARNING ⚠️ TensorBoard graph visualization failure Only tensors, lists, tuples of tensors, or dictionary of tensors can be output from traced functions\n",
            "       0/19      9.99G      6.127      4.972      5.208        307        640:   1%|          | 1/92 00:08/content/yolov9/train_dual.py:313: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(amp):\n",
            "       0/19        11G      6.878      5.497      5.273        732        640:   2%|▏         | 2/92 00:09Exception in thread Thread-23 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "       0/19        11G      7.217      6.132      5.398        471        640:   3%|▎         | 3/92 00:10Exception in thread Thread-24 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "       0/19        14G      7.356      5.139      5.256        273        640: 100%|██████████| 92/92 01:32\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:34\n",
            "                   all        735      23738          0          0          0          0\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/19        14G      6.943      4.735       5.06        514        640: 100%|██████████| 92/92 01:25\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738    0.00271     0.0252    0.00143   0.000433\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/19      13.1G      6.401       4.35      4.704        332        640: 100%|██████████| 92/92 01:25\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:32\n",
            "                   all        735      23738     0.0256      0.214     0.0154    0.00538\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/19      13.1G      5.254      3.627      3.836        549        640: 100%|██████████| 92/92 01:26\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738        0.1      0.186     0.0506     0.0184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/19      13.1G      4.128      2.899      3.041        288        640: 100%|██████████| 92/92 01:24\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.072      0.303     0.0505     0.0179\n",
            "Closing dataloader mosaic\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/19      13.1G      3.017      2.516      2.841        168        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.361      0.442      0.311      0.118\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/19      13.1G      2.783      2.256      2.462        204        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.393      0.474      0.366      0.149\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/19      13.1G      2.609       2.12      2.233        254        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.435      0.512      0.425      0.176\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/19      13.1G      2.588      2.062       2.11        190        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.403      0.514      0.401      0.187\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/19      13.1G      2.525      2.086      2.093        193        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.415       0.52      0.411      0.195\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/19      13.1G      2.458      1.979      1.974        285        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.473      0.518      0.452      0.184\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/19      13.1G      2.373      1.895      1.958        152        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.533      0.542      0.515      0.261\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/19      13.1G      2.392      1.901      1.954        203        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.558      0.564      0.545      0.246\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/19      13.1G      2.353      1.874      1.876        192        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738       0.56      0.565      0.554       0.28\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/19      13.1G      2.337      1.852      1.869        188        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.579      0.562      0.551      0.265\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/19      13.1G      2.309       1.83      1.873        233        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.572      0.564      0.536      0.235\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/19      13.1G      2.272      1.757      1.815        188        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738       0.57      0.572      0.528      0.235\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/19      13.1G      2.292      1.766      1.801        200        640: 100%|██████████| 92/92 01:17\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.616      0.572      0.583      0.282\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/19      13.1G       2.26      1.747      1.807        271        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:30\n",
            "                   all        735      23738      0.606      0.567      0.561      0.255\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/19      13.1G      2.203      1.722      1.772        155        640: 100%|██████████| 92/92 01:18\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:31\n",
            "                   all        735      23738      0.643        0.6      0.611      0.304\n",
            "\n",
            "20 epochs completed in 0.656 hours.\n",
            "/content/yolov9/utils/general.py:999: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  x = torch.load(f, map_location=torch.device('cpu'))\n",
            "Optimizer stripped from runs/train/yolov9-c/weights/last.pt, 102.8MB\n",
            "Optimizer stripped from runs/train/yolov9-c/weights/best.pt, 102.8MB\n",
            "\n",
            "Validating runs/train/yolov9-c/weights/best.pt...\n",
            "/content/yolov9/models/experimental.py:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ckpt = torch.load(attempt_download(w), map_location='cpu')  # load\n",
            "Fusing layers... \n",
            "yolov9-c summary: 604 layers, 50698278 parameters, 0 gradients, 236.6 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   2%|▏         | 1/46 00:00Exception in thread Thread-45 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "Exception in thread Thread-44 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   4%|▍         | 2/46 00:01Exception in thread Thread-47 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "Exception in thread Thread-46 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   7%|▋         | 3/46 00:02Exception in thread Thread-49 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "Exception in thread Thread-48 (plot_images):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 300, in plot_images\n",
            "    annotator.box_label(box, label, color=color)\n",
            "  File \"/content/yolov9/utils/plots.py\", line 86, in box_label\n",
            "    w, h = self.font.getsize(label)  # text width, height\n",
            "AttributeError: 'FreeTypeFont' object has no attribute 'getsize'\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 46/46 00:34\n",
            "                   all        735      23738      0.643      0.601      0.611      0.304\n",
            "Results saved to \u001b[1mruns/train/yolov9-c\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import copytree\n",
        "\n",
        "# Copy the 'runs' directory from yolov9 to your Google Drive\n",
        "source_directory = #your file path source\n",
        "destination_directory = #file path to where you want to save it\n",
        "\n",
        "# Copy the entire directory\n",
        "copytree(source_directory, destination_directory)\n"
      ],
      "metadata": {
        "id": "p3n1PTkXKMy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}